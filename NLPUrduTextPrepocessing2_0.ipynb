{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1Wjy2w9rxmOEevKboke1d7eJLQlnQlHj8",
      "authorship_tag": "ABX9TyOrmSTUEh0EYDJtVWBI5l+J",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SheikhMudassarHanif/NLP/blob/main/NLPUrduTextPrepocessing2_0.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "phase3 starting"
      ],
      "metadata": {
        "id": "2pGgV0Fhchaa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "\n"
      ],
      "metadata": {
        "id": "NNOleLYZcpqk"
      },
      "execution_count": 140,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk as nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qA4TRSsNLGVx",
        "outputId": "b4b558d2-2580-4198-d7ec-da66ad952b2d"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_stopWords(whitespaced_tokens, numpy_array):\n",
        "    # Create a new list to hold tokens after removing stop words\n",
        "    filtered_tokens = []\n",
        "\n",
        "    for word in whitespaced_tokens:\n",
        "        if word not in numpy_array:\n",
        "            filtered_tokens.append(word)  # Keep the word if it's not a stop word\n",
        "\n",
        "    return filtered_tokens"
      ],
      "metadata": {
        "id": "-B6zDXpvhcIM"
      },
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(text):\n",
        "    return text.translate(str.maketrans('', '', string.punctuation))"
      ],
      "metadata": {
        "id": "mv_4C_VhmQUj"
      },
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_hashtags(text):\n",
        "\n",
        "  return re.sub(r'#\\w+', '', text)\n"
      ],
      "metadata": {
        "id": "tQr1D8gO1ESK"
      },
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def remove_url(text):\n",
        "    return re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)"
      ],
      "metadata": {
        "id": "I6eZz0lEzECR"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: remove emjoi funcion\n",
        "\n",
        "import re\n",
        "\n",
        "def remove_emoji(text):\n",
        "  emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "  return emoji_pattern.sub(r'', text)\n"
      ],
      "metadata": {
        "id": "jU3fzlD0DV9H"
      },
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def Remove_short_convos(text):\n",
        "  positive_sentiments = [\"خوشی\", \"محبت\", \"امید\", \"عزت\", \"اطمینان\", \"رحمت\", \"خواب\", \"مسرت\", \"کامیابی\", \"سکون\", \"شکریہ\", \"دوستی\", \"خوبصورتی\", \"قوت\", \"خیرخواہی\"]\n",
        "  negative_sentiments = [\"غم\", \"نفرت\", \"مایوسی\", \"شرمندگی\", \"خوف\", \"تنہائی\", \"دکھ\", \"ناکامی\", \"پریشانی\", \"دھمکی\", \"نفرت\", \"خطرہ\", \"بے چینی\", \"دھوکہ\", \"غصہ\"]\n",
        "  if any(sentiment in text for sentiment in positive_sentiments) and any(sentiment in text for sentiment in negative_sentiments):\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        ""
      ],
      "metadata": {
        "id": "D41F7aozyzFF"
      },
      "execution_count": 182,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: import urduhack modules\n",
        "\n",
        "from urduhack import normalize\n",
        "from urduhack import tokenization\n"
      ],
      "metadata": {
        "id": "5Q5hR1mNxxaD"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text, stopwords_array):\n",
        "    # Step 1: Remove URLs\n",
        "    text = remove_url(text)\n",
        "\n",
        "    # Step 2: Remove Hashtags\n",
        "    text = remove_hashtags(text)\n",
        "\n",
        "    #step2.5 : Remove Emjoi\n",
        "    text=remove_emoji(text)\n",
        "\n",
        "    # Step 3: Remove Punctuation\n",
        "    text = remove_punctuation(text)\n",
        "\n",
        "    # Step 4: Tokenize text\n",
        "    # whitespaced_tokens = text.split()\n",
        "    whitespaced_tokens=nltk.word_tokenize(text)\n",
        "\n",
        "    # Step 5: Remove Stopwords\n",
        "    filtered_tokens = remove_stopWords(whitespaced_tokens, stopwords_array)\n",
        "\n",
        "    return ' '.join(filtered_tokens)\n",
        "\n",
        "\n",
        "\n",
        "data=pd.read_csv('/stopwords-list.csv')\n",
        "df = pd.DataFrame(data)\n",
        "urdu_stopwords = df.to_numpy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "corpus=[\n",
        "    'یہ کتاب بہت دلچسپ ہے اور مجھے بہت پسند ہے.',\n",
        "    '\"زندگی میں کامیابی کے لئے محنت اور صبر ضروری ہے۔\"',\n",
        "    'رنگ پلک سانپ\"',\n",
        "    'مجھے ابھی کام ختم نہیں ہوا۔\"',\n",
        "    \"حکومت نے بہت محنت کی اور کامیابی حاصل کی۔\",\n",
        "    \"سیاستدانوں نے عوام کے لئے سخت قوانین بنائے۔\",\n",
        "    \"بچے روزانہ کتابیں پڑھتے ہیں تاکہ وہ کامیاب ہوں۔\",\n",
        "    'کھاتے ہیں',\n",
        "    'طلباء سیاست پر بات کرتے ہیں اور کتابیں پڑھتے ہیں۔'\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "cleaned_corpus=[]\n",
        "for doc in corpus:\n",
        "  words = doc.split()\n",
        "  if len(words)<=3:\n",
        "    if Remove_short_convos(doc):\n",
        "      cleaned_corpus.append(clean_text(doc,urdu_stopwords))\n",
        "\n",
        "  else:\n",
        "      cleaned_corpus.append(clean_text(doc,urdu_stopwords))\n",
        "\n",
        "\n",
        "\n",
        "#cleaned corpus\n",
        "cleaned_corpus\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a5akeACpjPGV",
        "outputId": "46bb61ee-0299-4caf-b158-869edb87d9cf"
      },
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['کتاب دلچسپ پسند',\n",
              " 'زندگی کامیابی محنت صبر ضروری',\n",
              " 'کام ختم',\n",
              " 'حکومت محنت کامیابی حاصل',\n",
              " 'سیاستدانوں عوام سخت قوانین بنائے',\n",
              " 'بچے روزانہ کتابیں پڑھتے کامیاب',\n",
              " 'طلباء سیاست بات کتابیں پڑھتے']"
            ]
          },
          "metadata": {},
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_words = {\n",
        "    # General Nouns\n",
        "    'کتاب': ['کتاب', 'کتابیں'],                     # Book\n",
        "    'گھر': ['گھر', 'گھروں'],                         # House\n",
        "    'بچہ': ['بچہ', 'بچے', 'بچوں'],                   # Child\n",
        "    'نظام': ['نظام', 'نظاموں'],                     # System\n",
        "    'دنیا': ['دنیا', 'دنیاوں'],                      # World\n",
        "    'خواب': ['خواب', 'خوابوں'],                      # Dream\n",
        "    'دوست': ['دوست', 'دوستوں'],                      # Friend\n",
        "    'محبت': ['محبت', 'محبتوں'],                      # Love\n",
        "    'خوشی': ['خوشی', 'خوشیوں'],                      # Happiness\n",
        "    'دکھ': ['دکھ', 'دکھوں'],                          # Sadness\n",
        "    'فطرت': ['فطرت', 'فطرتوں'],                      # Nature\n",
        "    'کام': ['کام', 'کاموں'],                        # Work\n",
        "\n",
        "    # General Adjectives\n",
        "    'خوبصورت': ['خوبصورت', 'خوبصورتوں'],          # Beautiful\n",
        "    'اچھا': ['اچھا', 'اچھی', 'اچھے'],                # Good\n",
        "    'بڑا': ['بڑا', 'بڑی', 'بڑے'],                    # Big\n",
        "    'چھوٹا': ['چھوٹا', 'چھوٹی', 'چھوٹے'],           # Small\n",
        "    'مہنگا': ['مہنگا', 'مہنگی', 'مہنگے'],            # Expensive\n",
        "    'سستا': ['سستا', 'سستی', 'سستے'],               # Cheap\n",
        "    'سخت': ['سخت', 'سختی', 'سختیوں'],               # Hard\n",
        "\n",
        "    # General Verbs\n",
        "    'پڑھنا': ['پڑھنا', 'پڑھتے', 'پڑھتی', 'پڑھا'],   # To read\n",
        "    'لکھنا': ['لکھنا', 'لکھتے', 'لکھتی', 'لکھا'],    # To write\n",
        "    'چلنا': ['چلنا', 'چلتے', 'چلتی', 'چلا'],          # To walk\n",
        "    'دیکھنا': ['دیکھنا', 'دیکھتے', 'دیکھتی', 'دیکھا'], # To see\n",
        "    'سنا': ['سنا', 'سنتے', 'سنائی', 'سنایا'],         # To hear\n",
        "    'سمجھنا': ['سمجھنا', 'سمجھتے', 'سمجھتی', 'سمجھا'], # To understand\n",
        "    'کھیلنا': ['کھیلنا', 'کھیلتے', 'کھیلتی', 'کھیلا'], # To play\n",
        "    'کھانا': ['کھانا', 'کھاتے', 'کھاتی', 'کھایا'],     # To eat\n",
        "\n",
        "    # Political Nouns\n",
        "    'سیاست': ['سیاست', 'سیاستدان', 'سیاستدانوں'],    # Politics, Politician\n",
        "    'حکومت': ['حکومت', 'حکومتیں'],                     # Government\n",
        "    'پارٹی': ['پارٹی', 'پارٹیاں'],                      # Party\n",
        "    'انتخابات': ['انتخابات', 'انتخابی'],              # Elections\n",
        "    'حزب': ['حزب', 'احزاب'],                           # Party (political)\n",
        "    'قومی': ['قومی', 'قومیت'],                         # National\n",
        "    'مسئلہ': ['مسئلہ', 'مسائل'],                       # Issue\n",
        "    'بجٹ': ['بجٹ', 'بجٹوں'],                           # Budget\n",
        "    'مفاہمت': ['مفاہمت', 'مفاہمتیں'],                 # Reconciliation\n",
        "    'پالیسی': ['پالیسی', 'پالیسیوں'],                 # Policy\n",
        "    'غیرت': ['غیرت', 'غیرتیں'],                       # Honor\n",
        "    'دولت': ['دولت', 'دولتیں'],                       # Wealth\n",
        "    'قانون': ['قانون', 'قوانین'],                     # Law\n",
        "    'امن': ['امن', 'امنیں'],                          # Peace\n",
        "    'تنقید': ['تنقید', 'تنقیدیں'],                     # Criticism\n",
        "\n",
        "    # Sarcastic Expressions\n",
        "    'واہ': ['واہ', 'واہ واہ'],                       # Wow\n",
        "    'بہت اچھا': ['بہت اچھا', 'بہت اچھی', 'بہت اچھے'], # Very good (sarcastic)\n",
        "    'کمال': ['کمال', 'کمالات'],                      # Amazing (sarcastic)\n",
        "    'عمدہ': ['عمدہ', 'عمدہ کام'],                     # Excellent (sarcastic)\n",
        "    'کیا بات ہے': ['کیا بات ہے', 'کیا باتیں ہیں'],     # What a thing (sarcastic)\n",
        "    'زبردست': ['زبردست', 'زبردستی'],                 # Fantastic (sarcastic)\n",
        "    'مزا آ گیا': ['مزا آ گیا', 'مزا آگیا'],          # That was fun (sarcastic)\n",
        "    'شکریہ': ['شکریہ', 'شکریہ ادا کرنا'],             # Thank you (can be sarcastic)\n",
        "\n",
        "    # Political Adjectives\n",
        "    'سیاسی': ['سیاسی', 'سیاسیوں'],                    # Political\n",
        "    'مقامی': ['مقامی', 'مقامیوں'],                    # Local\n",
        "    'علاقائی': ['علاقائی', 'علاقائیوں'],              # Regional\n",
        "    'مفہومی': ['مفہومی', 'مفہومیوں'],                  # Conceptual\n",
        "    'مفاہمتی': ['مفاہمتی', 'مفاہمتیوں'],              # Reconciliatory\n",
        "    'مفید': ['مفید', 'مفیدوں'],                        # Useful\n",
        "    'نیک': ['نیک', 'نیکوں'],                            # Good/virtuous\n",
        "\n",
        "    # Political Verbs\n",
        "    'چننا': ['چننا', 'چنتے', 'چنتی', 'چنا'],            # To elect\n",
        "    'بیان کرنا': ['بیان کرنا', 'بیان کرتے', 'بیان کرتی'], # To state\n",
        "    'مذاکرہ کرنا': ['مذاکرہ کرنا', 'مذاکرے کرتے'],    # To negotiate\n",
        "    'منظور کرنا': ['منظور کرنا', 'منظور کرتے'],       # To approve\n",
        "    'تحریک کرنا': ['تحریک کرنا', 'تحریک کرتے'],       # To motivate\n",
        "    'اجلاس کرنا': ['اجلاس کرنا', 'اجلاس کرتے'],       # To convene\n",
        "    'تنقید کرنا': ['تنقید کرنا', 'تنقید کرتے'],        # To criticize\n",
        "    'تشکیل دینا': ['تشکیل دینا', 'تشکیل دیتے'],        # To form\n",
        "\n",
        "    # Other\n",
        "    'مسلم': ['مسلم', 'مسلمان', 'مسلمانوں'],            # Muslim\n",
        "    'عوام': ['عوام', 'عوامی'],                          # Public\n",
        "    'عقیدہ': ['عقیدہ', 'عقائد'],                       # Belief\n",
        "}\n",
        "\n",
        "# Example usage\n",
        "\n"
      ],
      "metadata": {
        "id": "1CdMIphMBJd2"
      },
      "execution_count": 195,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now lemmatization"
      ],
      "metadata": {
        "id": "AC0kFYg1DQ9_"
      },
      "execution_count": 196,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "updated_corpus = []\n",
        "\n",
        "for sentence in cleaned_corpus:\n",
        "    updated_sentence = []\n",
        "    words = sentence.split()  # Split sentence into individual words\n",
        "    for word in words:\n",
        "        found_root = False\n",
        "        for root_word, variants in base_words.items():\n",
        "            if word in variants:\n",
        "                updated_sentence.append(root_word)\n",
        "                found_root = True\n",
        "                break\n",
        "        if not found_root:\n",
        "            updated_sentence.append(word)  # If no root found, keep the original word\n",
        "    updated_corpus.append(' '.join(updated_sentence))\n",
        "\n",
        "# Print the updated corpus with root words\n",
        "for sentence in updated_corpus:\n",
        "    print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vPcEnGELd9o",
        "outputId": "f9f1af88-18d3-4a45-e266-1fcdba9753c7"
      },
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کتاب دلچسپ پسند\n",
            "زندگی کامیابی محنت صبر ضروری\n",
            "کام ختم\n",
            "حکومت محنت کامیابی حاصل\n",
            "سیاست عوام سخت قانون بنائے\n",
            "بچہ روزانہ کتاب پڑھنا کامیاب\n",
            "طلباء سیاست بات کتاب پڑھنا\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from LughaatNLP import LughaatNLP"
      ],
      "metadata": {
        "id": "M7Hun_5_5Sdw"
      },
      "execution_count": 204,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urdu_text_processing = LughaatNLP()\n"
      ],
      "metadata": {
        "id": "TJPFVFhSAB6-"
      },
      "execution_count": 205,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmed_sentence=[]\n",
        "\n",
        "for sentence in cleaned_corpus:\n",
        "  stemmed_sentence.append(urdu_text_processing.urdu_stemmer(sentence))\n",
        ""
      ],
      "metadata": {
        "id": "gl1Jh7RVBXaY"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in stemmed_sentence:\n",
        "  print(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mkLCNK8BsqZ",
        "outputId": "55bef00a-b252-41b8-bfa0-24ce00512a9a"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کتب دلچسپ پسند\n",
            "زندگی کامیابی محنت صبر ضروری\n",
            "کام ختم\n",
            "حکومت محنت کامیابی حاصل\n",
            "سیاستدانا عوام سخت قوانین بنائہ\n",
            "بچہ روزانہ کتب پڑھ کامیاب\n",
            "طلبء سیاست به کتب پڑھ\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Phase 3 start feature extraction"
      ],
      "metadata": {
        "id": "T4NHmIv7DQm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#tokenization step 1\n",
        "vocabulary=[]\n",
        "for sentence in updated_corpus:\n",
        "  words=sentence.split()\n",
        "  for word in words:\n",
        "    if word not in vocabulary:\n",
        "      vocabulary.append(word)"
      ],
      "metadata": {
        "id": "kt5bHHyeB4Hy"
      },
      "execution_count": 219,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_data=pd.read_csv('/content/drive/MyDrive/urdu_sarcastic_dataset.csv')"
      ],
      "metadata": {
        "id": "txdkKWqeDYoD"
      },
      "execution_count": 217,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# file_data.head()"
      ],
      "metadata": {
        "id": "NIA9voxREt5r"
      },
      "execution_count": 218,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for words in vocabulary:\n",
        "  print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wE8D20u7EwyX",
        "outputId": "086615d5-c121-43c6-b686-e3c0099e5c04"
      },
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "کتاب\n",
            "دلچسپ\n",
            "پسند\n",
            "زندگی\n",
            "کامیابی\n",
            "محنت\n",
            "صبر\n",
            "ضروری\n",
            "کام\n",
            "ختم\n",
            "حکومت\n",
            "حاصل\n",
            "سیاست\n",
            "عوام\n",
            "سخت\n",
            "قانون\n",
            "بنائے\n",
            "بچہ\n",
            "روزانہ\n",
            "پڑھنا\n",
            "کامیاب\n",
            "طلباء\n",
            "بات\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mDIJFrjfFDnx"
      },
      "execution_count": 222,
      "outputs": []
    }
  ]
}